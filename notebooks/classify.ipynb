{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\fyp3\\lib\\site-packages\\malaya_boilerplate\\frozen_graph.py:35: UserWarning: Cannot import beam_search_ops from Tensorflow Addons, ['malaya.jawi_rumi.deep_model', 'malaya.phoneme.deep_model', 'malaya.rumi_jawi.deep_model', 'malaya.stem.deep_model'] will not available to use, make sure Tensorflow Addons version >= 0.12.0\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\envs\\fyp3\\lib\\site-packages\\malaya_boilerplate\\frozen_graph.py:38: UserWarning: check compatible Tensorflow version with Tensorflow Addons at https://github.com/tensorflow/addons/releases\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\anaconda3\\envs\\fyp3\\lib\\site-packages\\malaya\\tokenizer.py:202: FutureWarning: Possible nested set at position 3361\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n",
      "c:\\Users\\User\\anaconda3\\envs\\fyp3\\lib\\site-packages\\malaya\\tokenizer.py:202: FutureWarning: Possible nested set at position 3879\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import malaya\n",
    "import re\n",
    "from statistics import mean\n",
    "import pickle\n",
    "\n",
    "mly = malaya.sentiment.multinomial()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "malay_stopwords = np.load('data/malay_stopwords.npy')\n",
    "positive_malay_words = np.load('data/positive_malay_words.npy')\n",
    "negative_malay_words = np.load('data/negative_malay_words.npy')\n",
    "positive_word_top_100 = np.load('data/positive_word_top_100.npy')\n",
    "negative_word_top_100 = np.load('data/negative_word_top_100.npy')\n",
    "learned_words = np.load('data/learned_words.npy')\n",
    "\n",
    "def get_features(tweet):\n",
    "    features = dict()\n",
    "    top_positive_words_frequency = 0\n",
    "    top_negative_words_frequency = 0\n",
    "    positive_malay_words_frequency = 0\n",
    "    negative_malay_words_frequency = 0\n",
    "    english_compound_scores = list()\n",
    "    english_positive_scores = list()\n",
    "    english_negative_scores = list()\n",
    "    malaya_positive_scores = list()\n",
    "    malaya_negative_scores = list()\n",
    "\n",
    "    word_tokens = set(nltk.word_tokenize(tweet))\n",
    "\n",
    "    for sentence in nltk.sent_tokenize(tweet):\n",
    "        #removes hyperlinks and twitter mentions\n",
    "        sentence = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','', sentence)\n",
    "        sentence = re.sub('(@[A-Za-z0-9_]+)','', sentence)\n",
    "\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word.lower() in positive_word_top_100:\n",
    "                top_positive_words_frequency += 1\n",
    "            if word.lower() in negative_word_top_100:\n",
    "                top_negative_words_frequency += 1\n",
    "            if word.lower() in positive_malay_words:\n",
    "                positive_malay_words_frequency += 1\n",
    "            if word.lower() in negative_malay_words:\n",
    "                negative_malay_words_frequency += 1\n",
    "        \n",
    "        malaya_positive_scores.append(mly.predict_proba([sentence])[0][\"positive\"])\n",
    "        malaya_negative_scores.append(mly.predict_proba([sentence])[0][\"negative\"])\n",
    "\n",
    "        english_compound_scores.append(sia.polarity_scores(sentence)[\"compound\"])\n",
    "        english_positive_scores.append(sia.polarity_scores(sentence)[\"pos\"])\n",
    "        english_negative_scores.append(sia.polarity_scores(sentence)[\"neg\"])\n",
    "\n",
    "    features[\"malaya_mean_positive\"] = mean(malaya_positive_scores)\n",
    "    features[\"malaya_mean_negative\"] = mean(malaya_negative_scores)\n",
    "    \n",
    "    features[\"english_mean_compound\"] = mean(english_compound_scores) + 1\n",
    "    features[\"english_mean_positive\"] = mean(english_positive_scores)\n",
    "    features[\"english_mean_negative\"] = mean(english_negative_scores)\n",
    "    \n",
    "    features[\"top_positive_words_frequency\"] = top_positive_words_frequency\n",
    "    features[\"top_negative_words_frequency\"] = top_negative_words_frequency\n",
    "    \n",
    "    features[\"positive_malay_words_frequency\"] = positive_malay_words_frequency\n",
    "    features[\"negative_malay_words_frequency\"] = negative_malay_words_frequency\n",
    "\n",
    "    for word in learned_words:\n",
    "        features[F\"contains({word})\"] = (word in word_tokens)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tweet = \"\"\n",
    "\n",
    "loaded_classifier = pickle.load(open('models/MLPClassifier.pickle', 'rb'))\n",
    "loaded_classifier.classify(get_features(new_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_classifier = pickle.load(open('models/MLPClassifier.pickle', 'rb'))\n",
    "\n",
    "df = pd.read_csv('datasets/duck_training.csv')\n",
    "df = df.replace(-1, 'negative')\n",
    "df = df.replace(0, 'neutral')\n",
    "df = df.replace(1, 'positive')\n",
    "df = df[df['TweetSentiment'] != 'neutral']\n",
    "\n",
    "df2 = pd.read_csv('datasets/student_tweet_training.csv')\n",
    "\n",
    "df3 = pd.read_csv('datasets/malay_dataset_twitter_training.csv')\n",
    "df3 = df3[df3['TweetSentiment'] != 'neutral']\n",
    "\n",
    "dfs = [df3]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True) #shuffles the dataframe\n",
    "\n",
    "no_neutrals = df[df['TweetSentiment'] != 'neutral']\n",
    "sentiment = np.array(no_neutrals['TweetSentiment'])\n",
    "tweets = np.array(no_neutrals['TweetText'])\n",
    "\n",
    "scorelist = []\n",
    "results = []\n",
    "for tweet in tweets:\n",
    "    #scores = get_features(tweet)\n",
    "    prediction = loaded_classifier.classify(get_features(tweet))\n",
    "    \n",
    "    #scorelist.append(scores)\n",
    "    results.append(prediction)\n",
    "\n",
    "correct = 0\n",
    "for i in range(0, len(results)):\n",
    "    print(F\"\\nTweet: {tweets[i]}\")\n",
    "    #print(F\"Scores: {scorelist[i]}\")\n",
    "    print(F\"Prediction: {results[i]} | Sentiment: {sentiment[i]}\")\n",
    "\n",
    "    if results[i] == sentiment[i]:\n",
    "        correct += 1\n",
    "\n",
    "total = len(results)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(F\"\\nTotal: {total} | Correct: {correct}\")\n",
    "print(F\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_classifier = pickle.load(open('models/MLPClassifier.pickle', 'rb'))\n",
    "\n",
    "df = pd.read_csv('datasets/duck_training.csv')\n",
    "df = df.replace(-1, 'negative')\n",
    "df = df.replace(0, 'neutral')\n",
    "df = df.replace(1, 'positive')\n",
    "df = df[df['TweetSentiment'] != 'neutral']\n",
    "\n",
    "df2 = pd.read_csv('datasets/student_tweet_training.csv')\n",
    "\n",
    "df3 = pd.read_csv('datasets/malay_dataset_twitter_training.csv')\n",
    "df3 = df3[df3['TweetSentiment'] != 'neutral']\n",
    "\n",
    "dfs = [df3]\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True) #shuffles the dataframe\n",
    "\n",
    "no_neutrals = df[df['TweetSentiment'] != 'neutral']\n",
    "sentiment = np.array(no_neutrals['TweetSentiment'])\n",
    "tweets = np.array(no_neutrals['TweetText'])\n",
    "\n",
    "results = []\n",
    "for tweet in tweets:\n",
    "    prediction = loaded_classifier.classify(get_features(tweet))\n",
    "    results.append(prediction)\n",
    "\n",
    "wrong = 0\n",
    "for i in range(0, len(results)):\n",
    "    if results[i] != sentiment[i]:\n",
    "        print(F\"\\nTweet: {tweets[i]}\")\n",
    "        print(F\"Prediction: {results[i]} | Sentiment: {sentiment[i]}\")\n",
    "        wrong += 1\n",
    "\n",
    "total = len(results)\n",
    "print(F\"\\nTotal Predictions: {total} | Wrong Predictions: {wrong}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd06832646357e8a5987645492eb6c1833a8582ca1f5e56bdc62040d9f1db677"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
